nbdiff /tmp/cjFLfl_Data_preprocessing_and_model_building (part I)-checkpoint.ipynb .ipynb_checkpoints/Data_preprocessing_and_model_building (part I)-checkpoint.ipynb
--- /tmp/cjFLfl_Data_preprocessing_and_model_building (part I)-checkpoint.ipynb  2019-10-31 08:46:45.125632
+++ .ipynb_checkpoints/Data_preprocessing_and_model_building (part I)-checkpoint.ipynb  2019-10-31 08:43:15.451463
[34m## inserted before /cells/0:[0m
[32m+  code cell:
[32m+    source:
[32m+      !pip3 install scikit-multilearn
[32m+  markdown cell:
[32m+    source:
[32m+      Run `wandb login` from the terminal after installing `wandb`. 
[32m+  markdown cell:
[32m+    source:
[32m+      ## Imports

[0m[34m## deleted /cells/0:[0m
[31m-  markdown cell:
[31m-    source:
[31m-      **To be included**:
[31m-      - Comments
[31m-      - References
[31m-      
[31m-      This notebook shows the use of simpler models to classify the titles. 

[0m[34m## inserted before /cells/1/outputs/0:[0m
[32m+  output:
[32m+    output_type: stream
[32m+    name: stderr
[32m+    text:
[32m+      [nltk_data] Downloading package stopwords to
[32m+      [nltk_data]     /home/jupyter/nltk_data...
[32m+      [nltk_data]   Package stopwords is already up-to-date!

[0m[34m## modified /cells/1/source:[0m
[36m@@ -4,18 +4,13 @@[m [mfrom sklearn.linear_model import LogisticRegression[m
 from sklearn.multiclass import OneVsRestClassifier[m
 from sklearn.naive_bayes import MultinomialNB[m
 from sklearn.metrics import accuracy_score[m
[31m-from sklearn.utils import class_weight[m
 from sklearn.pipeline import Pipeline[m
 from sklearn.svm import LinearSVC[m
 from skmultilearn.problem_transform import LabelPowerset[m
 from nltk.corpus import stopwords[m
 from ast import literal_eval[m
[31m-from typing import Union[m
[31m-import matplotlib.pyplot as plt[m
[32m+[m[32mfrom utils import utils[m
 import numpy as np[m
 import sklearn[m
 import wandb[m
[31m-import nltk[m
[31m-import time[m
[31m-import re[m
[31m-import io[m
[m
[32m+[m[32mimport time[m
[m

[0m[34m## inserted before /cells/2:[0m
[32m+  markdown cell:
[32m+    source:
[32m+      ## Data loading
[32m+  code cell:
[32m+    execution_count: 2
[32m+    source:
[32m+      def load_data(filename:str) -> np.ndarray:
[32m+          content = np.load(filename, allow_pickle=True)
[32m+          return content

[0m[34m## deleted /cells/2:[0m
[31m-  code cell:
[31m-    execution_count: 2
[31m-    source:
[31m-      %matplotlib inline
[31m-      nltk.download('stopwords')
[31m-    outputs:
[31m-      output 0:
[31m-        output_type: stream
[31m-        name: stderr
[31m-        text:
[31m-          [nltk_data] Downloading package stopwords to
[31m-          [nltk_data]     /Users/sayakpaul/nltk_data...
[31m-          [nltk_data]   Package stopwords is already up-to-date!
[31m-      output 1:
[31m-        output_type: execute_result
[31m-        execution_count: 2
[31m-        data:
[31m-          text/plain: True

[0m[34m## replaced /cells/3/execution_count:[0m
[31m-  2
[32m+  3

[0m[34m## replaced /cells/3/outputs/0/execution_count:[0m
[31m-  2
[32m+  3

[0m[34m## modified /cells/3/source:[0m
[36m@@ -1,4 +1,4 @@[m
[31m-X_train, y_train = np.load('data/X_train.npy', allow_pickle=True), np.load('data/y_train.npy', allow_pickle=True)[m
[31m-X_test, y_test = np.load('data/X_test.npy', allow_pickle=True), np.load('data/y_test.npy', allow_pickle=True)[m
[32m+[m[32mX_train, y_train = load_data('data/X_train.npy'), load_data('data/y_train.npy')[m
[32m+[m[32mX_test, y_test = load_data('data/X_test.npy'), load_data('data/y_test.npy')[m
 [m
 X_train.shape, X_test.shape[m
[m

[0m[34m## inserted before /cells/4:[0m
[32m+  markdown cell:
[32m+    source:
[32m+      ## Cleaning the paper titles
[32m+  code cell:
[32m+    execution_count: 4
[32m+    source:
[32m+      clean_title = np.vectorize(utils.clean_title)

[0m[34m## deleted /cells/4:[0m
[31m-  code cell:
[31m-    execution_count: 3
[31m-    source:
[31m-      wpt = nltk.WordPunctTokenizer()
[31m-      stop_words = nltk.corpus.stopwords.words('english')
[31m-      
[31m-      def clean_title(title):
[31m-          # lower case and remove special characters\whitespaces
[31m-          title = re.sub(r'[^a-zA-Z\s]', '', title, re.I|re.A)
[31m-          title = title.lower()
[31m-          title = title.strip()
[31m-          # tokenize document
[31m-          tokens = wpt.tokenize(title)
[31m-          # filter stopwords out of document
[31m-          filtered_tokens = [token for token in tokens if token not in stop_words]
[31m-          # re-create document from filtered tokens
[31m-          title = ' '.join(filtered_tokens)
[31m-          return title
[31m-      
[31m-      clean_title = np.vectorize(clean_title)

[0m[34m## replaced /cells/5/execution_count:[0m
[31m-  4
[32m+  5

[0m[34m## replaced /cells/6/execution_count:[0m
[31m-  5
[32m+  6

[0m[34m## replaced /cells/6/outputs/0/execution_count:[0m
[31m-  5
[32m+  6

[0m[34m## modified /cells/6/source:[0m
[36m@@ -1 +1,2 @@[m
[32m+[m[32m# Preview[m
 X_train[:10][m
[m

[0m[34m## replaced /cells/7/execution_count:[0m
[31m-  6
[32m+  7

[0m[34m## replaced /cells/7/outputs/0/execution_count:[0m
[31m-  6
[32m+  7

[0m[34m## inserted before /cells/8:[0m
[32m+  markdown cell:
[32m+    source:
[32m+      ## Label preprocessing

[0m[34m## replaced /cells/8/execution_count:[0m
[31m-  7
[32m+  8

[0m[34m## replaced /cells/8/outputs/0/execution_count:[0m
[31m-  7
[32m+  8

[0m[34m## replaced /cells/9/execution_count:[0m
[31m-  8
[32m+  9

[0m[34m## replaced /cells/9/outputs/0/execution_count:[0m
[31m-  8
[32m+  9

[0m[34m## replaced /cells/10/execution_count:[0m
[31m-  9
[32m+  10

[0m[34m## replaced /cells/10/outputs/0/execution_count:[0m
[31m-  9
[32m+  10

[0m[34m## inserted before /cells/11:[0m
[32m+  markdown cell:
[32m+    source:
[32m+      Make sure you don't call `fit_transform` on mlb when you are applying it on the test set as it will disturb the training/serving skew. 
[32m+  markdown cell:
[32m+    source:
[32m+      ## Initial configurations before modeling

[0m[34m## replaced /cells/11/execution_count:[0m
[31m-  10
[32m+  11

[0m[34m## modified /cells/11/source:[0m
[36m@@ -1 +1,2 @@[m
[32m+[m[32m# Gather the English stopwords[m
 stop_words = set(stopwords.words('english'))[m
[m

[0m[34m## inserted before /cells/12:[0m
[32m+  code cell:
[32m+    execution_count: 12
[32m+    source:
[32m+      # Initialize wandb to log your experiments
[32m+      wandb.init(project="arxiv-project-simple-models")
[32m+    outputs:
[32m+      output 0:
[32m+        output_type: display_data
[32m+        data:
[32m+          text/html:
[32m+            
[32m+                            Logging results to <a href="https://wandb.com" target="_blank">Weights & Biases</a>.<br/>
[32m+                            Project page: <a href="https://app.wandb.ai/sayakpaul/arxiv-project-simple-models" target="_blank">https://app.wandb.ai/sayakpaul/arxiv-project-simple-models</a><br/>
[32m+                            Run page: <a href="https://app.wandb.ai/sayakpaul/arxiv-project-simple-models/runs/4211t4bq" target="_blank">https://app.wandb.ai/sayakpaul/arxiv-project-simple-models/runs/4211t4bq</a><br/>
[32m+                            Docs: <a href="https://docs.wandb.com/integrations/jupyter.html" target="_blank">https://docs.wandb.com/integrations/jupyter.html</a><br/>
[32m+                        
[32m+          text/plain: <IPython.core.display.HTML object>
[32m+      output 1:
[32m+        output_type: execute_result
[32m+        execution_count: 12
[32m+        data:
[32m+          text/plain: W&B Run: https://app.wandb.ai/sayakpaul/arxiv-project-simple-models/runs/4211t4bq
[32m+  markdown cell:
[32m+    source:
[32m+      ## Modeling experiments
[32m+  markdown cell:
[32m+    source:
[32m+      We will define a helper function to train ane evaluate a model. Before you begin the training process you might want to perform any left-over preprocessing. `scikit-learn` makes it easier to define these steps along with your desired machine learning models with _Pipelines_. So, the input to our helper function would be a pipeline. 

[0m[34m## deleted /cells/12:[0m
[31m-  code cell:
[31m-    execution_count: 11
[31m-    source:
[31m-      wandb.init(project="data-preprocessing-and-model-building-1")
[31m-    outputs:
[31m-      output 0:
[31m-        output_type: display_data
[31m-        data:
[31m-          text/html:
[31m-            
[31m-                    Notebook configured with <a href="https://wandb.com" target="_blank">W&B</a>. You can <a href="https://app.wandb.ai/sayakpaul/data-preprocessing-and-model-building-1/runs/nkgewsba?apiKey=ab6c21fa98b79790d8c7a9d0a0844b7a1ddaa4e2" target="_blank">open</a> the run page, or call <code>%%wandb</code>
[31m-                    in a cell containing your training loop to display live results.  Learn more in our <a href="https://docs.wandb.com/docs/integrations/jupyter.html" target="_blank">docs</a>.
[31m-                
[31m-          text/plain: <IPython.core.display.HTML object>
[31m-      output 1:
[31m-        output_type: execute_result
[31m-        execution_count: 11
[31m-        data:
[31m-          text/plain: W&B Run: https://app.wandb.ai/sayakpaul/data-preprocessing-and-model-building-1/runs/nkgewsba?apiKey=ab6c21fa98b79790d8c7a9d0a0844b7a1ddaa4e2

[0m[34m## replaced /cells/13/execution_count:[0m
[31m-  12
[32m+  13

[0m[34m## modified /cells/13/source:[0m
[36m@@ -8,4 +8,4 @@[m [mdef train_eval_pipeline(pipeline:sklearn.pipeline, train_data:tuple,[m
     prediction = pipeline.predict(X_test)[m
     wandb.log({"accuracy":accuracy_score(y_test_binarized, prediction)*100.0,\[m
                "training_time":end})[m
[31m-    return f'Accuracy score: {accuracy_score(y_test_binarized, prediction)*100.0}%, model trained in {end} secs'[m
[m
[32m+[m[32m    return 'Accuracy score: {}%, model trained in {} secs'.format(accuracy_score(y_test_binarized, prediction)*100.0, end)[m
[m

[0m[34m## inserted before /cells/14:[0m
[32m+  markdown cell:
[32m+    source:
[32m+      As we are dealing with a mult-label classification problem, we will need to wrap any binary classifier in `OneVsRestClassifier`. Let's start off with a naive Bayes model. 

[0m[34m## replaced /cells/14/execution_count:[0m
[31m-  16
[32m+  14

[0m[34m## deleted /cells/14/outputs/0:[0m
[31m-  output:
[31m-    output_type: stream
[31m-    name: stderr
[31m-    text:
[31m-      wandb: Wandb version 0.8.12 is available!  To upgrade, please run:
[31m-      wandb:  $ pip install wandb --upgrade

[0m[34m## modified /cells/14/outputs/1/text:[0m
[36m@@ -1 +1 @@[m
[31m-Accuracy score: 35.484857754665036%, model trained in 0.6247339248657227 secs[m
[32m+[m[32mAccuracy score: 35.484857754665036%, model trained in 0.6501975059509277 secs[m

[0m[34m## modified /cells/14/source:[0m
[36m@@ -1,3 +1,4 @@[m
[32m+[m[32m# naive Bayes[m
 nb_pipeline = Pipeline([[m
                 ('tfidf', TfidfVectorizer(stop_words=stop_words)),[m
                 ('clf', OneVsRestClassifier(MultinomialNB([m

[0m[34m## modified /metadata/language_info/version:[0m
[31m-  3.7.1
[32m+  3.5.3

[0m[34m## replaced /nbformat_minor:[0m
[31m-  2
[32m+  4

[0mnbdiff /tmp/4Y4MnZ_Data_preprocessing_and_model_building (part I).ipynb Data_preprocessing_and_model_building (part I).ipynb
--- /tmp/4Y4MnZ_Data_preprocessing_and_model_building (part I).ipynb  2019-10-31 08:46:45.697680
+++ Data_preprocessing_and_model_building (part I).ipynb  2019-10-31 08:45:16.233983
[34m## inserted before /cells/0:[0m
[32m+  code cell:
[32m+    source:
[32m+      !pip3 install scikit-multilearn
[32m+  markdown cell:
[32m+    source:
[32m+      Run `wandb login` from the terminal after installing `wandb`. 
[32m+  markdown cell:
[32m+    source:
[32m+      ## Imports

[0m[34m## deleted /cells/0:[0m
[31m-  markdown cell:
[31m-    source:
[31m-      **To be included**:
[31m-      - Comments
[31m-      - References
[31m-      
[31m-      This notebook shows the use of simpler models to classify the titles. 

[0m[34m## inserted before /cells/1/outputs/0:[0m
[32m+  output:
[32m+    output_type: stream
[32m+    name: stderr
[32m+    text:
[32m+      [nltk_data] Downloading package stopwords to
[32m+      [nltk_data]     /home/jupyter/nltk_data...
[32m+      [nltk_data]   Package stopwords is already up-to-date!

[0m[34m## modified /cells/1/source:[0m
[36m@@ -4,18 +4,13 @@[m [mfrom sklearn.linear_model import LogisticRegression[m
 from sklearn.multiclass import OneVsRestClassifier[m
 from sklearn.naive_bayes import MultinomialNB[m
 from sklearn.metrics import accuracy_score[m
[31m-from sklearn.utils import class_weight[m
 from sklearn.pipeline import Pipeline[m
 from sklearn.svm import LinearSVC[m
 from skmultilearn.problem_transform import LabelPowerset[m
 from nltk.corpus import stopwords[m
 from ast import literal_eval[m
[31m-from typing import Union[m
[31m-import matplotlib.pyplot as plt[m
[32m+[m[32mfrom utils import utils[m
 import numpy as np[m
 import sklearn[m
 import wandb[m
[31m-import nltk[m
[31m-import time[m
[31m-import re[m
[31m-import io[m
[m
[32m+[m[32mimport time[m
[m

[0m[34m## inserted before /cells/2:[0m
[32m+  markdown cell:
[32m+    source:
[32m+      ## Data loading
[32m+  code cell:
[32m+    execution_count: 2
[32m+    source:
[32m+      def load_data(filename:str) -> np.ndarray:
[32m+          content = np.load(filename, allow_pickle=True)
[32m+          return content

[0m[34m## deleted /cells/2:[0m
[31m-  code cell:
[31m-    execution_count: 2
[31m-    source:
[31m-      %matplotlib inline
[31m-      nltk.download('stopwords')
[31m-    outputs:
[31m-      output 0:
[31m-        output_type: stream
[31m-        name: stderr
[31m-        text:
[31m-          [nltk_data] Downloading package stopwords to
[31m-          [nltk_data]     /Users/sayakpaul/nltk_data...
[31m-          [nltk_data]   Package stopwords is already up-to-date!
[31m-      output 1:
[31m-        output_type: execute_result
[31m-        execution_count: 2
[31m-        data:
[31m-          text/plain: True

[0m[34m## replaced /cells/3/execution_count:[0m
[31m-  2
[32m+  3

[0m[34m## replaced /cells/3/outputs/0/execution_count:[0m
[31m-  2
[32m+  3

[0m[34m## modified /cells/3/source:[0m
[36m@@ -1,4 +1,4 @@[m
[31m-X_train, y_train = np.load('data/X_train.npy', allow_pickle=True), np.load('data/y_train.npy', allow_pickle=True)[m
[31m-X_test, y_test = np.load('data/X_test.npy', allow_pickle=True), np.load('data/y_test.npy', allow_pickle=True)[m
[32m+[m[32mX_train, y_train = load_data('data/X_train.npy'), load_data('data/y_train.npy')[m
[32m+[m[32mX_test, y_test = load_data('data/X_test.npy'), load_data('data/y_test.npy')[m
 [m
 X_train.shape, X_test.shape[m
[m

[0m[34m## inserted before /cells/4:[0m
[32m+  markdown cell:
[32m+    source:
[32m+      ## Cleaning the paper titles
[32m+  code cell:
[32m+    execution_count: 4
[32m+    source:
[32m+      clean_title = np.vectorize(utils.clean_title)

[0m[34m## deleted /cells/4:[0m
[31m-  code cell:
[31m-    execution_count: 3
[31m-    source:
[31m-      wpt = nltk.WordPunctTokenizer()
[31m-      stop_words = nltk.corpus.stopwords.words('english')
[31m-      
[31m-      def clean_title(title):
[31m-          # lower case and remove special characters\whitespaces
[31m-          title = re.sub(r'[^a-zA-Z\s]', '', title, re.I|re.A)
[31m-          title = title.lower()
[31m-          title = title.strip()
[31m-          # tokenize document
[31m-          tokens = wpt.tokenize(title)
[31m-          # filter stopwords out of document
[31m-          filtered_tokens = [token for token in tokens if token not in stop_words]
[31m-          # re-create document from filtered tokens
[31m-          title = ' '.join(filtered_tokens)
[31m-          return title
[31m-      
[31m-      clean_title = np.vectorize(clean_title)

[0m[34m## replaced /cells/5/execution_count:[0m
[31m-  4
[32m+  5

[0m[34m## replaced /cells/6/execution_count:[0m
[31m-  5
[32m+  6

[0m[34m## replaced /cells/6/outputs/0/execution_count:[0m
[31m-  5
[32m+  6

[0m[34m## modified /cells/6/source:[0m
[36m@@ -1 +1,2 @@[m
[32m+[m[32m# Preview[m
 X_train[:10][m
[m

[0m[34m## replaced /cells/7/execution_count:[0m
[31m-  6
[32m+  7

[0m[34m## replaced /cells/7/outputs/0/execution_count:[0m
[31m-  6
[32m+  7

[0m[34m## inserted before /cells/8:[0m
[32m+  markdown cell:
[32m+    source:
[32m+      ## Label preprocessing

[0m[34m## replaced /cells/8/execution_count:[0m
[31m-  7
[32m+  8

[0m[34m## replaced /cells/8/outputs/0/execution_count:[0m
[31m-  7
[32m+  8

[0m[34m## replaced /cells/9/execution_count:[0m
[31m-  8
[32m+  9

[0m[34m## replaced /cells/9/outputs/0/execution_count:[0m
[31m-  8
[32m+  9

[0m[34m## replaced /cells/10/execution_count:[0m
[31m-  9
[32m+  10

[0m[34m## replaced /cells/10/outputs/0/execution_count:[0m
[31m-  9
[32m+  10

[0m[34m## inserted before /cells/11:[0m
[32m+  markdown cell:
[32m+    source:
[32m+      Make sure you don't call `fit_transform` on mlb when you are applying it on the test set as it will disturb the training/serving skew. 
[32m+  markdown cell:
[32m+    source:
[32m+      ## Initial configurations before modeling

[0m[34m## replaced /cells/11/execution_count:[0m
[31m-  10
[32m+  11

[0m[34m## modified /cells/11/source:[0m
[36m@@ -1 +1,2 @@[m
[32m+[m[32m# Gather the English stopwords[m
 stop_words = set(stopwords.words('english'))[m
[m

[0m[34m## inserted before /cells/12:[0m
[32m+  code cell:
[32m+    execution_count: 12
[32m+    source:
[32m+      # Initialize wandb to log your experiments
[32m+      wandb.init(project="arxiv-project-simple-models")
[32m+    outputs:
[32m+      output 0:
[32m+        output_type: display_data
[32m+        data:
[32m+          text/html:
[32m+            
[32m+                            Logging results to <a href="https://wandb.com" target="_blank">Weights & Biases</a>.<br/>
[32m+                            Project page: <a href="https://app.wandb.ai/sayakpaul/arxiv-project-simple-models" target="_blank">https://app.wandb.ai/sayakpaul/arxiv-project-simple-models</a><br/>
[32m+                            Run page: <a href="https://app.wandb.ai/sayakpaul/arxiv-project-simple-models/runs/4211t4bq" target="_blank">https://app.wandb.ai/sayakpaul/arxiv-project-simple-models/runs/4211t4bq</a><br/>
[32m+                            Docs: <a href="https://docs.wandb.com/integrations/jupyter.html" target="_blank">https://docs.wandb.com/integrations/jupyter.html</a><br/>
[32m+                        
[32m+          text/plain: <IPython.core.display.HTML object>
[32m+      output 1:
[32m+        output_type: execute_result
[32m+        execution_count: 12
[32m+        data:
[32m+          text/plain: W&B Run: https://app.wandb.ai/sayakpaul/arxiv-project-simple-models/runs/4211t4bq
[32m+  markdown cell:
[32m+    source:
[32m+      ## Modeling experiments
[32m+  markdown cell:
[32m+    source:
[32m+      We will define a helper function to train ane evaluate a model. Before you begin the training process you might want to perform any left-over preprocessing. `scikit-learn` makes it easier to define these steps along with your desired machine learning models with _Pipelines_. So, the input to our helper function would be a pipeline. 

[0m[34m## deleted /cells/12:[0m
[31m-  code cell:
[31m-    execution_count: 11
[31m-    source:
[31m-      wandb.init(project="data-preprocessing-and-model-building-1")
[31m-    outputs:
[31m-      output 0:
[31m-        output_type: display_data
[31m-        data:
[31m-          text/html:
[31m-            
[31m-                    Notebook configured with <a href="https://wandb.com" target="_blank">W&B</a>. You can <a href="https://app.wandb.ai/sayakpaul/data-preprocessing-and-model-building-1/runs/nkgewsba?apiKey=ab6c21fa98b79790d8c7a9d0a0844b7a1ddaa4e2" target="_blank">open</a> the run page, or call <code>%%wandb</code>
[31m-                    in a cell containing your training loop to display live results.  Learn more in our <a href="https://docs.wandb.com/docs/integrations/jupyter.html" target="_blank">docs</a>.
[31m-                
[31m-          text/plain: <IPython.core.display.HTML object>
[31m-      output 1:
[31m-        output_type: execute_result
[31m-        execution_count: 11
[31m-        data:
[31m-          text/plain: W&B Run: https://app.wandb.ai/sayakpaul/data-preprocessing-and-model-building-1/runs/nkgewsba?apiKey=ab6c21fa98b79790d8c7a9d0a0844b7a1ddaa4e2

[0m[34m## replaced /cells/13/execution_count:[0m
[31m-  12
[32m+  13

[0m[34m## modified /cells/13/source:[0m
[36m@@ -8,4 +8,4 @@[m [mdef train_eval_pipeline(pipeline:sklearn.pipeline, train_data:tuple,[m
     prediction = pipeline.predict(X_test)[m
     wandb.log({"accuracy":accuracy_score(y_test_binarized, prediction)*100.0,\[m
                "training_time":end})[m
[31m-    return f'Accuracy score: {accuracy_score(y_test_binarized, prediction)*100.0}%, model trained in {end} secs'[m
[m
[32m+[m[32m    return 'Accuracy score: {}%, model trained in {} secs'.format(accuracy_score(y_test_binarized, prediction)*100.0, end)[m
[m

[0m[34m## inserted before /cells/14:[0m
[32m+  markdown cell:
[32m+    source:
[32m+      As we are dealing with a mult-label classification problem, we will need to wrap any binary classifier in `OneVsRestClassifier`. Let's start off with a naive Bayes model. 

[0m[34m## replaced /cells/14/execution_count:[0m
[31m-  16
[32m+  14

[0m[34m## deleted /cells/14/outputs/0:[0m
[31m-  output:
[31m-    output_type: stream
[31m-    name: stderr
[31m-    text:
[31m-      wandb: Wandb version 0.8.12 is available!  To upgrade, please run:
[31m-      wandb:  $ pip install wandb --upgrade

[0m[34m## modified /cells/14/outputs/1/text:[0m
[36m@@ -1 +1 @@[m
[31m-Accuracy score: 35.484857754665036%, model trained in 0.6247339248657227 secs[m
[32m+[m[32mAccuracy score: 35.484857754665036%, model trained in 0.6501975059509277 secs[m

[0m[34m## modified /cells/14/source:[0m
[36m@@ -1,3 +1,4 @@[m
[32m+[m[32m# naive Bayes[m
 nb_pipeline = Pipeline([[m
                 ('tfidf', TfidfVectorizer(stop_words=stop_words)),[m
                 ('clf', OneVsRestClassifier(MultinomialNB([m

[0m[34m## replaced /cells/15/execution_count:[0m
[31m-  14
[32m+  15

[0m[34m## deleted /cells/15/outputs/0:[0m
[31m-  output:
[31m-    output_type: stream
[31m-    name: stderr
[31m-    text:
[31m-      wandb: Wandb version 0.8.12 is available!  To upgrade, please run:
[31m-      wandb:  $ pip install wandb --upgrade

[0m[34m## modified /cells/15/outputs/1/text:[0m
[36m@@ -1 +1 @@[m
[31m-Accuracy score: 45.71734475374733%, model trained in 2.155222177505493 secs[m
[32m+[m[32mAccuracy score: 45.71734475374733%, model trained in 2.001997947692871 secs[m

[0m[34m## modified /cells/15/source:[0m
[36m@@ -1,3 +1,4 @@[m
[32m+[m[32m# SVM[m
 svc_pipeline = Pipeline([[m
                 ('tfidf', TfidfVectorizer(stop_words=stop_words)),[m
                 ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),[m

[0m[34m## modified /cells/16/outputs/0/text:[0m
[36m@@ -1,4 +1,2 @@[m
[31m-/miniconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.[m
[32m+[m[32m/home/jupyter/.local/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.[m
   FutureWarning)[m
[31m-wandb: Wandb version 0.8.12 is available!  To upgrade, please run:[m
[31m-wandb:  $ pip install wandb --upgrade[m

[0m[34m## modified /cells/16/outputs/1/text:[0m
[36m@@ -1 +1 @@[m
[31m-Accuracy score: 42.82655246252677%, model trained in 2.4853322505950928 secs[m
[32m+[m[32mAccuracy score: 42.81125726521872%, model trained in 4.470703363418579 secs[m

[0m[34m## modified /cells/16/source:[0m
[36m@@ -1,3 +1,4 @@[m
[32m+[m[32m# Logistic Regression[m
 lr_pipeline = Pipeline([[m
                 ('tfidf', TfidfVectorizer(stop_words=stop_words)),[m
                 ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs=1)),[m

[0mdiff --git a/utils/.ipynb_checkpoints/utils-checkpoint.py b/utils/.ipynb_checkpoints/utils-checkpoint.py
index 1521362..62b5cec 100644
--- a/utils/.ipynb_checkpoints/utils-checkpoint.py
+++ b/utils/.ipynb_checkpoints/utils-checkpoint.py
@@ -1,11 +1,16 @@
+from sklearn.preprocessing import MultiLabelBinarizer
 import numpy as np
+import pickle
 import nltk
+import re
+
+nltk.download('stopwords')
 
 wpt = nltk.WordPunctTokenizer()
 stop_words = nltk.corpus.stopwords.words('english')
 
 # Function courtesy: http://bit.ly/2N17Clt
-def clean_title(title: ):
+def clean_title(title: np.ndarray) -> np.ndarray:
     # lower case and remove special characters\whitespaces
     title = re.sub(r'[^a-zA-Z\s]', '', title, re.I|re.A)
     title = title.lower()
@@ -16,4 +21,5 @@ def clean_title(title: ):
     filtered_tokens = [token for token in tokens if token not in stop_words]
     # re-create document from filtered tokens
     title = ' '.join(filtered_tokens)
-    return title
\ No newline at end of file
+    return title
+    
\ No newline at end of file
diff --git a/utils/utils.py b/utils/utils.py
index 1521362..62b5cec 100644
--- a/utils/utils.py
+++ b/utils/utils.py
@@ -1,11 +1,16 @@
+from sklearn.preprocessing import MultiLabelBinarizer
 import numpy as np
+import pickle
 import nltk
+import re
+
+nltk.download('stopwords')
 
 wpt = nltk.WordPunctTokenizer()
 stop_words = nltk.corpus.stopwords.words('english')
 
 # Function courtesy: http://bit.ly/2N17Clt
-def clean_title(title: ):
+def clean_title(title: np.ndarray) -> np.ndarray:
     # lower case and remove special characters\whitespaces
     title = re.sub(r'[^a-zA-Z\s]', '', title, re.I|re.A)
     title = title.lower()
@@ -16,4 +21,5 @@ def clean_title(title: ):
     filtered_tokens = [token for token in tokens if token not in stop_words]
     # re-create document from filtered tokens
     title = ' '.join(filtered_tokens)
-    return title
\ No newline at end of file
+    return title
+    
\ No newline at end of file
diff --git a/wandb/settings b/wandb/settings
index 355f26e..02c4222 100644
--- a/wandb/settings
+++ b/wandb/settings
@@ -1,3 +1,6 @@
 [default]
-base_url: https://api.wandb.ai
-anonymous: False
+base_url = https://api.wandb.ai
+anonymous = False
+entity = sayakpaul
+project = arxiv-project-simple-models
+
